This is the foundation. We build a local model training pipeline with tracking and artifact management, ready to extend into full deployment.

# 1. Repository Scaffold
We set up a clean structure so training, deployment, and infra code live in predictable places:
```bash
ml-deploy-pipeline/
├─ notebooks/           # Jupyter notebooks for exploration + training
├─ src/                 # Source code for training and utilities
├─ artifacts/           # Local output folder for models + metrics
├─ mlruns/              # Local MLflow tracking (autogenerated)
├─ requirements.txt     # Python dependencies
├─ .gitignore
└─ README.md
```

# 2. Training Workflow
- Used the Iris dataset from scikit-learn (no download needed).
- Trained a `RandomForestClassifier` as a simple baseline model.
- Split data into train/test sets.
- Evaluated accuracy and macro-F1 score.

# 3. Experiment Tracking (MLflow)
- MLflow is configured locally with tracking stored under `mlruns/`.
- Logs include hyperparameters, metrics, and model artifacts.
- Each run saves outputs in `artifacts/model_<timestamp>/`.

# 4. Artifacts
- Each training run produces:
- `model.joblib` → serialized sklearn model
- `metrics.json` → accuracy, f1 scores, etc.
- `model_info.json` → model metadata (type, params, features, timestamp, MLflow run ID)

# 5. Scripts and Notebook
- `notebooks/01_train.ipynb` → reproducible notebook version of training
- `src/train.py` → script version for CI/CD
- `src/utils.py` and `src/data_loader.py` → helpers for splitting data and saving artifacts

# Requirements
Install Dependencies
```bash
pip install -r requirements.txt
```

Running training script
```bash
python -m src.train
```

**OR** open Jupyter and run
```bash
notebooks/01_train.ipynb
```
